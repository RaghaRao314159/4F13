\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\citation{bishop2006pattern}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Task (a) - Maximum Likelihood (ML) Multinomial}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Top 20 words in the Kos training dataset. The most common word, ``bush'', has 1.4\% probability.}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:taska_bar}{{1}{1}{Top 20 words in the Kos training dataset. The most common word, ``bush'', has 1.4\% probability}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Task (b) - Bayesian Inference with Dirichlet Prior}{2}{section.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces As $\alpha $ increases, all probabilities converge to uniform $1/W$. Small $\alpha $ preserves ML structure while avoiding zero probabilities.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:taskb_alpha}{{2}{2}{As $\alpha $ increases, all probabilities converge to uniform $1/W$. Small $\alpha $ preserves ML structure while avoiding zero probabilities}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Task (c) - Test Document Log Probability and Perplexity}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Perplexity varies widely across documents. Doc 2001 has unusually high perplexity, suggesting atypical vocabulary.}}{2}{figure.3}\protected@file@percent }
\newlabel{fig:taskc_perp}{{3}{2}{Perplexity varies widely across documents. Doc 2001 has unusually high perplexity, suggesting atypical vocabulary}{figure.3}{}}
\citation{blei2003latent}
\@writefile{toc}{\contentsline {section}{\numberline {4}Task (d) - BMM Gibbs Sampling}{3}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces BMM Gibbs sampling ($K=20$, $\alpha =10$, $\gamma =0.1$). Left: Mixing proportions over sweeps. Middle: Rolling std dev (convergence). Right: Final proportions.}}{3}{figure.4}\protected@file@percent }
\newlabel{fig:taskd}{{4}{3}{BMM Gibbs sampling ($K=20$, $\alpha =10$, $\gamma =0.1$). Left: Mixing proportions over sweeps. Middle: Rolling std dev (convergence). Right: Final proportions}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Task (e) - Latent Dirichlet Allocation}{3}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces LDA ($K=20$, 50 sweeps). Left: Topic proportions stabilize by sweep 30. Right: Word entropy (bits) decreases as topics become focused.}}{3}{figure.5}\protected@file@percent }
\newlabel{fig:taske_evol}{{5}{3}{LDA ($K=20$, 50 sweeps). Left: Topic proportions stabilize by sweep 30. Right: Word entropy (bits) decreases as topics become focused}{figure.5}{}}
\bibcite{bishop2006pattern}{1}
\bibcite{blei2003latent}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Final state: topics have balanced proportions (0.03 to 0.08) and varying specialization (entropy 8 to 10 bits).}}{4}{figure.6}\protected@file@percent }
\newlabel{fig:taske_final}{{6}{4}{Final state: topics have balanced proportions (0.03 to 0.08) and varying specialization (entropy 8 to 10 bits)}{figure.6}{}}
\gdef \@abspage@last{4}
